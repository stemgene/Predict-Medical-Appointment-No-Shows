{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine learning based on the no show data.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stemgene/Predict-Medical-Appointment-No-Shows/blob/master/Machine_learning_based_on_the_no_show_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdFff6Lis-9R",
        "colab_type": "text"
      },
      "source": [
        "# Introduction:\n",
        "\n",
        "In this kernel, we tried to make the classification of label of \"no show\" as the processes below:\n",
        "\n",
        "* Data preprocessing\n",
        "* Split data, and address over-sampling using SMOTE\n",
        "* Use Logistic Regresson, Random Forest, and SVM to train the model\n",
        "* Use neural network to train the model\n",
        "\n",
        "### Result:\n",
        "Model performance was evaluated by precision, recall, and accuracy. The table below summarizes the model performance on the validation and testing set. As can be seen from this table, logistic regression and SVM did not perform well on our sample, even after hyperparameter tuning. Random forest model produced the best results, with a precision of 0.83, a recall of 0.73, and an accuracy of 0.91 on the testing set. Although the neural network resulted in a good performance on the validation set after fine-tuning, the precision and recall decreased to 0.47 at testing time. Results of the sensitivity analysis were included in the Appendix, where we did not observe significant differences from our main analysis.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>  </th>\n",
        "    <th colspan='2'><center>Linear Regression</center></th>\n",
        "    <th colspan='2'><center>SVM</center></th>\n",
        "    <th colspan='2'><center>Random Forest</center></th>\n",
        "    <th colspan='2'><center>Neural Network</center></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> Dataset </td>\n",
        "    <td>Validation Set</td>\n",
        "    <td>Testing Set</td>\n",
        "    <td>Validation Set</td>\n",
        "    <td>Testing Set</td>\n",
        "    <td>Validation Set</td>\n",
        "    <td>Testing Set</td>\n",
        "    <td>Validation Set</td>\n",
        "    <td>Testing Set</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Precision</td>\n",
        "    <td><center>0.32</center></td>\n",
        "    <td><center>0.31</center></td>\n",
        "    <td><center>0.29</center></td>\n",
        "    <td><center>0.3</center></td>\n",
        "    <td><center>0.82</center></td>\n",
        "    <td><center>0.83</center></td>\n",
        "    <td><center>0.84</center></td>\n",
        "    <td><center>0.47</center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Recall</td>\n",
        "    <td><center>0.59</center></td>\n",
        "    <td><center>0.59</center></td>\n",
        "    <td><center>0.88</center></td>\n",
        "    <td><center>0.89</center></td>\n",
        "    <td><center>0.72</center></td>\n",
        "    <td><center>0.73</center></td>\n",
        "    <td><center>0.78</center></td>\n",
        "    <td><center>0.47</center></td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Accuracy</td>\n",
        "    <td><center>0.66</center></td>\n",
        "    <td><center>0.66</center></td>\n",
        "    <td><center>0.54</center></td>\n",
        "    <td><center>0.54</center></td>\n",
        "    <td><center>0.91</center></td>\n",
        "    <td><center>0.91</center></td>\n",
        "    <td><center>0.82</center></td>\n",
        "    <td><center>0.78</center></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "We performed variable importance analysis with logistic regression and random forest. The results are shown in the table below. These two models produced consistent variable importance, indicating that lead time, age, previous no-show rate, receiving reminder messages, and male gender are strong predictors of no-show outcome. Specifically, from Table 3 we can see that patients with long lead time, with diabetes and alcoholism, and having a high previous no-show rate were more likely to be no-show patients. Male patients and older patients were more likely to attend their appointments. Receiving reminder messages was positively associated with the outcome, one interpretation is that the reminder messages may only have been sent to patients who had nonattendance history.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th><center>Variables</center></th>\n",
        "    <th><center>Coef.</center></th>\n",
        "    <th><center>p_value</center></th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Lead_Time</td>\n",
        "    <td>0.0271</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Diabetes</td>\n",
        "    <td>0.1022</td>\n",
        "    <td>0.0004</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Alcoholism</td>\n",
        "    <td>0.2656</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>SMS_Received</td>\n",
        "    <td>0.3538</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Male</td>\n",
        "    <td>-0.1676</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Age_scaled</td>\n",
        "    <td>-1.3964</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Noshow_Rate</td>\n",
        "    <td>-0.6419</td>\n",
        "    <td>0</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPjrewSltFCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "df = pd.read_csv(\"../input/noshowappointments/KaggleV2-May-2016.csv\", )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBuJnloAtCVt",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZuNHkzCtOIJ",
        "colab_type": "text"
      },
      "source": [
        "We summarized all features into six categories. The features of “patient ID” and “appointment ID” identify a patient and a record of the appointment. The patient’s demographic category shows gender, neighbourhood, and age. There are two timestamp variables, “scheduled day” and “appointment day”. We also have some binary data, which indicate patients’ medical conditions, insurance, and whether the patient receives a reminder message from the hospital. As a dataset of classification, the label feature is “No Show”, which is a binary variable, it shows whether the patient is no-show or not.\n",
        "* Index: PatientId, AppointmentID\n",
        "* Categorical: Gender, Neighborhood\n",
        "* Time stamp: ScheduledDay, AppointmentDay\n",
        "* Numerical: Age\n",
        "* Binary: Alcoholism, Hipertension, Scholarship, Diabetes, Handcap, SMS_received\n",
        "* Label: No-show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90-D9QUHtQ1I",
        "colab_type": "text"
      },
      "source": [
        "### 1. ScheduledDay and AppointmentDay\n",
        "\n",
        "previous literature indicates that lead time (time interval between schedule day and the actual appointment day) is one of the strong predictors of no-show, we thus constructed this variable using the variables ScheduleDay and AppointmentDay in our dataset. Another important predictor is the day of week, which may affect patient’s behavior. Therefore, we created a new feature “day of week” using the variable AppointmentDay.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVX6xL9-tUhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Insert the 5th column that indicate the weekday of AppointmentDay\n",
        "# dt.weekday: Monday-0, Friday-4, Sunday-6\n",
        "df.insert(5, \"Appoint Weekday\", df.loc[:, 'AppointmentDay'].apply(np.datetime64).dt.weekday)\n",
        "\n",
        "# Transform str to pd.datestamp and date form\n",
        "df.loc[:, 'ScheduledDay'] = df.loc[:, 'ScheduledDay'].apply(np.datetime64).dt.date\n",
        "df.loc[:, 'AppointmentDay'] = df.loc[:, 'AppointmentDay'].apply(np.datetime64).dt.date\n",
        "\n",
        "# Insert the 6th column that indicate the difference between scheduled date and appointment date\n",
        "date_difference = []\n",
        "for i in range(len(df.loc[:, 'ScheduledDay'])):\n",
        "  diff = (df.loc[:, 'AppointmentDay'][i]-df.loc[:, 'ScheduledDay'][i]).days\n",
        "  date_difference.append(diff)\n",
        "df.insert(6, \"Date Difference\", date_difference)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is3uJ6HDtWnZ",
        "colab_type": "text"
      },
      "source": [
        "### 2. Location Information: \"Neighborhood\"\n",
        "\n",
        "Patients’ geographical information was specified to the town level in our dataset. There are 81 towns in Victoria, a city in Brazil. \n",
        "\n",
        "We got every town's GPS address from Google Map API, however, if Google cannot search information by the input, it will return the address of upper level, which is the state of ES, Brazil. We had to pick these wrong results and check them by hand.\n",
        "\n",
        "In this notebook, we omit this session, instead, we upload the file containing the GPS information and frequency of appointment of every town."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjGDKGxFtVqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "towns_df = pd.read_csv(\"../input/no-show/towns.csv\")\n",
        "towns = {row[0]:[row[1],float(row[2]),float(row[3])] for index, row in towns_df.iterrows()} # load file is string type, need to transform float\n",
        "len(towns)  #79, already drop 2 observations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Kw8ucPtcmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_df = df[[\"PatientId\", 'Neighbourhood']]\n",
        "# delete extreme values\n",
        "cluster_df.drop(cluster_df.index[cluster_df['Neighbourhood'] == 'ILHAS OCEÂNICAS DE TRINDADE'], inplace = True)\n",
        "cluster_df.drop(cluster_df.index[cluster_df['Neighbourhood'] == 'PARQUE INDUSTRIAL'], inplace = True)\n",
        "whole_towns_gps = [(towns[key][1], towns[key][2]) for key in cluster_df['Neighbourhood'].values]\n",
        "cluster_df.insert(2,\"town gps\", whole_towns_gps)\n",
        "cluster_df.tail(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRdgwPbotgE4",
        "colab_type": "text"
      },
      "source": [
        "#### visualize the patient distribution\n",
        "\n",
        "If we zoom in the figure shown below, we can find many towns contain less than 500 patients. In total of 81 towns, we want to merge small towns into adjacent towns and combine them into larger groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEGAQyF1te3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyepsg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFyAd-J9tkqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import geopandas as gpd\n",
        "import pyepsg\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from folium.plugins import MarkerCluster\n",
        "locationlist = [(v[0],v[1]) for v in cluster_df['town gps'].values]\n",
        "map2 = folium.Map(location=[-20.268857, -40.302106], tiles='cartodbdark_matter', zoom_start=13)\n",
        "marker_cluster = MarkerCluster().add_to(map2)\n",
        "\n",
        "for point in range(0, 5000):  # The maximum number is 5000, if the number is greater, this map won't show up. Anyone can tell me what's wrong with it?\n",
        "    folium.Marker(locationlist[point]).add_to(marker_cluster)\n",
        "map2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PyhmxWPtp2S",
        "colab_type": "text"
      },
      "source": [
        "#### K-means\n",
        "\n",
        "\n",
        "We use K-means to merge nearby towns automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T82xLoHWtnM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()  # for plot styling\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X = np.array([[v[1], v[2]] for v in towns.values()])\n",
        "kmeans = KMeans(n_clusters=30)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.scatter(X[:, 1], X[:, 0], c=y_kmeans, s=30, cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWlmIWYhttRd",
        "colab_type": "text"
      },
      "source": [
        "We certainly can introduce K-means like this. However, the result is not good enough, since this method just merge points depend on their locations, but ignore the importance or count number of patients of a town. If we compare it with the first figure above, some small towns such as \"ILHA DO FRADE\", which is an island, are clusters alone. \n",
        "\n",
        "We want to merge these towns not only by distance, but also by the weight or count number of patients.\n",
        "\n",
        "Therefore we synthesize virtual points for each town according the count number of patients, and apply K-means again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JLw7Wyotsuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# synthesize virtual points for each town according the count number of patients\n",
        "# distance of 0.001(gps) = 110m\n",
        "np.random.seed(0)\n",
        "cov = ([-3e-8, 0],[0, 3e-8]) # range of random\n",
        "patient_gps = [np.random.multivariate_normal(v,  cov , 1) for v in cluster_df['town gps'].values]\n",
        "cluster_df.insert(3,\"patient gps\", patient_gps)\n",
        "cluster_df.tail(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcKFf9SHty4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()  # for plot styling\n",
        "import numpy as np\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "X = np.array([(v[0][0],v[0][1]) for v in cluster_df['patient gps'].values])\n",
        "kmeans = KMeans(n_clusters=15)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "\n",
        "figure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\n",
        "plt.scatter(X[:, 1], X[:, 0], c=y_kmeans, s=30, cmap='viridis', edgecolor='None')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guGr04CPt2Q0",
        "colab_type": "text"
      },
      "source": [
        "We can see small towns are merged into other groups. \n",
        "\n",
        "Finally we got 15 clusters, and insert this column to dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0UF5d9oyrt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop=['ILHAS OCEÂNICAS DE TRINDADE', 'PARQUE INDUSTRIAL'] #delete two extreme values, which cannot be found on Google map\n",
        "df = df[~df['Neighbourhood'].isin(drop)]\n",
        "df.insert(9, \"K-means cluster\", y_kmeans)\n",
        "#delete some wrong data from last session\n",
        "df = df[df[\"Date Difference\"] >= 0] \n",
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVMixf7WyuSu",
        "colab_type": "text"
      },
      "source": [
        "### 3. One-hot coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVXZr6X3yy1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Male'] = np.where(df['Gender']== \"M\", 1, 0)\n",
        "df['Label'] = np.where(df['No-show']==\"Yes\", 1, 0)\n",
        "cat_columns = ['Handcap','Appoint Weekday', 'K-means cluster']\n",
        "df = pd.get_dummies(df, prefix_sep=\"__\", columns=cat_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImhGb8WEy0Hg",
        "colab_type": "text"
      },
      "source": [
        "### 4.Age\n",
        "\n",
        "we removed patients under 0-year-old and normalized the variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqo--Ewry2nD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 28866(26%) patients are under 18, probably not appropriate to drop them\n",
        "df['under18'] =  np.where(df['Age'].astype(int)>18, \"no\", \"yes\" )\n",
        "print(df['under18'].value_counts())\n",
        "\n",
        "# delete rows with age<0\n",
        "df.drop(df[df['Age'].astype(int) < 0].index, inplace = True) \n",
        "hist = df['Age'].astype(int).hist(bins=100)\n",
        "hist\n",
        "\n",
        "# normalize (instead of grouping, i remember back in class jack said grouping age will cause information loss)\n",
        "from sklearn import preprocessing\n",
        "age = df[['Age']].values.astype(int)\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "df['age_scaled'] =  min_max_scaler.fit_transform(age)\n",
        " \n",
        "boolean = any(df.duplicated())\n",
        "print(\"any duplicates: \" , boolean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHVE7AEyy6ZK",
        "colab_type": "text"
      },
      "source": [
        "### 5. Noshow History\n",
        "\n",
        "From some literature, a patient’s no-show history would be a very important feature. Hence we calculated the no-show rate before current patient visit based on “Patient ID” and “No-show” label together. Noteworthily, we created a new feature to distinguish new patients with other patients who truly had zero no-show rate for previous visits (clean sheet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIc9ucv7y96H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## check\n",
        "df['Label'].value_counts()\n",
        "## distinct PatientId count\n",
        "len(df['PatientId'].unique())  # 62296\n",
        "\n",
        "df_visit=df.sort_values(by=['PatientId','ScheduledDay'])\n",
        "\n",
        "##  visit\n",
        "df_visit['Visit_Nth']=df_visit.groupby('PatientId').cumcount()+1\n",
        "df_visit['New_Visit']=df_visit['Visit_Nth'].apply(lambda x: 1 if x==1 else 0)\n",
        "df_visit['Noshow_cumsum']=df_visit.groupby('PatientId')['Label'].cumsum()\n",
        "\n",
        "### noshow rate \n",
        "def noshow_rate(df):\n",
        "    if df['Visit_Nth']==1:\n",
        "        rate = 0    \n",
        "    elif df['Visit_Nth']>1:\n",
        "        if df['Label']==1:\n",
        "            rate = (df['Noshow_cumsum']-1 )/ (df['Visit_Nth'] -1 )\n",
        "        else:\n",
        "            rate = (df['Noshow_cumsum']  )/ (df['Visit_Nth'] -1 )       \n",
        "    return rate\n",
        "\n",
        "df_visit['Noshow_rate']= df_visit.apply(noshow_rate, axis=1)\n",
        "\n",
        "## max\n",
        "df_max= pd.DataFrame(df_visit.groupby('PatientId')['Visit_Nth'].max()).reset_index()\n",
        "df_max = df_max.rename(columns={'Visit_Nth':'Visit_Nth_max'} )\n",
        "df_max.head()\n",
        "\n",
        "## check unique patients\n",
        "print(df_max.shape,\"\\n\",df_max['Visit_Nth_max'].describe())\n",
        "df_max['Visit_Nth_max'].value_counts()\n",
        "\n",
        "## hist\n",
        "df_max.hist(bins=20,label='Visit_Nth_max');\n",
        "plt.xlabel(\"Visit_Nth_max\")\n",
        "plt.ylabel(\"Patients\")\n",
        "plt.title(\"Distribution of Visit_Nth_max\")\n",
        "plt.show()\n",
        "\n",
        "## outlier defined\n",
        "cut_list = [0, 0.25, 0.5, 0.75, 0.85, 0.9, 0.95, 0.99, 1]\n",
        "cut_Visit_Nth_max = df_max['Visit_Nth_max'].quantile(q=cut_list)\n",
        "print(cut_Visit_Nth_max)\n",
        "## decide to drop top 1%: keep Visit_Nth_max <=8\n",
        "\n",
        "## merge\n",
        "df_fin= df_visit.merge(df_max, on='PatientId', how='left')\n",
        "df_fin.shape\n",
        "\n",
        "## check\n",
        "df_fin.tail(10)\n",
        "\n",
        "## check\n",
        "(df_fin[df_fin.Noshow_rate>0 & (df_fin.Noshow_rate<1)]).tail(10)\n",
        "\n",
        "## check\n",
        "df_fin[df_fin.PatientId=='99966898398164']\n",
        "\n",
        "df= df_fin.copy()\n",
        "\n",
        "## drop outlier\n",
        "df= df[df['Visit_Nth_max']<=8]\n",
        "\n",
        "## only keep Noshow_rate and New_Visit\n",
        "df = df.drop(['Visit_Nth','Noshow_cumsum','Visit_Nth_max'], axis=1)\n",
        "\n",
        "boolean = any(df.duplicated())\n",
        "print(\"any duplicates: \" , boolean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMVjuCI9zDjY",
        "colab_type": "text"
      },
      "source": [
        "# Splitting data by PatientId, Over-sampling using SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWkSVa6VzFos",
        "colab_type": "text"
      },
      "source": [
        "There were 22,319 (20.19%) missed appointments in our dataset, indicating the issue of class imbalance. We doubled the observations in the minority class and randomly sampled an equal number of observations in the majority class to address this issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_kxEyrJzBK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import seaborn as sns\n",
        "\n",
        "# splitting by patientid\n",
        "_, id_indices = np.unique(np.array(df['PatientId']), return_inverse=True)\n",
        "train_ids, val_test_ids = next(GroupShuffleSplit(test_size=.30, n_splits=2, random_state = 8).split(df, groups=id_indices))\n",
        "\n",
        "val_test_set = df.iloc[val_test_ids]\n",
        "_, id_indices = np.unique(np.array(val_test_set['PatientId']), return_inverse=True)\n",
        "val_ids, test_ids = next(GroupShuffleSplit(test_size=.50, n_splits=2, random_state = 8).split(val_test_set , groups=id_indices))\n",
        "\n",
        "train_set = df.iloc[train_ids]\n",
        "val_set =  df.iloc[val_ids]\n",
        "test_set =  df.iloc[test_ids]\n",
        "\n",
        "boolean = any(train_set.duplicated())\n",
        "print(\"train_set any duplicates: \" , boolean)\n",
        "\n",
        "boolean = any(val_set.duplicated())\n",
        "print(\"val_set any duplicates: \" , boolean)\n",
        "\n",
        "boolean = any(test_set.duplicated())\n",
        "print(\"test_set any duplicates: \" , boolean)\n",
        "      \n",
        "# drops = ['No-show',\"Gender\",'AppointmentID', 'PatientId', 'ScheduledDay','AppointmentDay',  'Age','under18', 'Neighbourhood']\n",
        "# train_set = train_set.drop(drops, axis=1)\n",
        "# val_set = val_set.drop(drops, axis=1)\n",
        "# test_set = test_set.drop(drops, axis=1)\n",
        "\n",
        "# Check whether there are any duplicate values in different data sets.\n",
        "check =  any(item in train_set['PatientId'] for item in (val_set['PatientId']))\n",
        "if check:\n",
        "  print(\"train_set['PatientId'] contains some elements of val_set['PatientId']\")      \n",
        "else:\n",
        "  print(\"train_set['PatientId'] does not have any element of val_set['PatientId'].\")\n",
        "\n",
        "check =  any(item in train_set['PatientId'] for item in (test_set['PatientId']))\n",
        "if check:\n",
        "  print(\"train_set['PatientId'] contains some elements of test_set['PatientId']\")      \n",
        "else:\n",
        "  print(\"train_set['PatientId'] does not have any element of test_set['PatientId'].\")\n",
        "\n",
        "check =  any(item in val_set['PatientId'] for item in (test_set['PatientId']))\n",
        "if check:\n",
        "  print(\"val_set['PatientId'] contains some elements of test_set['PatientId']\")      \n",
        "else:\n",
        "  print(\"val_set['PatientId'] does not have any element of test_set['PatientId'].\")\n",
        "\n",
        "boolean = any(train_set.duplicated())\n",
        "print(\"train_set any duplicates: \" , boolean)\n",
        "\n",
        "boolean = any(val_set.duplicated())\n",
        "print(\"val_set any duplicates: \" , boolean)\n",
        "\n",
        "boolean = any(test_set.duplicated())\n",
        "print(\"test_set any duplicates: \" , boolean)\n",
        "\n",
        "drops = ['Label','No-show',\"Gender\",'AppointmentID', 'PatientId', 'ScheduledDay','AppointmentDay', 'Age','under18', 'Neighbourhood']\n",
        "X_train = train_set.drop(drops, axis=1, index=None)\n",
        "y_train = train_set['Label']\n",
        "\n",
        "X_val = val_set.drop(drops, axis=1)\n",
        "y_val = val_set['Label']\n",
        "\n",
        "X_test = test_set.drop(drops, axis=1)\n",
        "y_test = test_set['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx1QoF5MzKVD",
        "colab_type": "text"
      },
      "source": [
        "#### Resample the minority class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2YktyJNzMoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sm = SMOTE(sampling_strategy='minority',random_state=8)\n",
        "\n",
        "\n",
        "X_train_os, y_train_os = sm.fit_sample(X_train, y_train)  \n",
        "X_train_os = pd.DataFrame(X_train_os)\n",
        "X_train_os.columns = X_train.columns\n",
        "y_train_os = pd.DataFrame(y_train_os)\n",
        "y_train_os.columns = ['Label']\n",
        "boolean = any(X_train_os.duplicated())\n",
        "print(\"\\nX_train_os any duplicates: \" , boolean)\n",
        "\n",
        "\n",
        "\n",
        "X_val_os, y_val_os = sm.fit_sample(X_val, y_val)\n",
        "X_val_os = pd.DataFrame(X_val_os)\n",
        "X_val_os.columns = X_val.columns\n",
        "y_val_os = pd.DataFrame(y_val_os)\n",
        "y_val_os.columns = ['Label']\n",
        "\n",
        "\n",
        "X_test_os, y_test_os = sm.fit_sample(X_test, y_test)\n",
        "X_test_os = pd.DataFrame(X_test_os)\n",
        "X_test_os.columns = X_test.columns\n",
        "y_test_os = pd.DataFrame(y_test_os)\n",
        "y_test_os.columns = ['Label']\n",
        "\n",
        "\n",
        "train_set_os = pd.concat([X_train_os, y_train_os], axis=1)\n",
        "val_set_os = pd.concat([X_val_os, y_val_os], axis=1)\n",
        "test_set_os = pd.concat([X_test_os, y_test_os], axis=1)\n",
        "\n",
        "print('\\ntrain_set: ',train_set.shape, \"val_set: \", val_set.shape, \"test_set:\", test_set.shape)\n",
        "print('\\ntrain_set_os: ',train_set_os.shape, \"val_set_os: \", val_set_os.shape, \"test_set_os:\", test_set_os.shape)\n",
        "\n",
        "boolean = any(train_set_os.duplicated())\n",
        "print(\"train_set_os any duplicates: \" , boolean)\n",
        "\n",
        "boolean = any(val_set_os.duplicated())\n",
        "print(\"val_set_os any duplicates: \" , boolean)\n",
        "\n",
        "boolean = any(test_set.duplicated())\n",
        "print(\"test_set_os any duplicates: \" , boolean)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n6pLzLnzPaJ",
        "colab_type": "text"
      },
      "source": [
        "# Use Logistic Regresson, Random Forest, and SVM to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nucfYaAbzSFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features  = ['Date Difference', 'Scholarship', 'Hipertension', 'Diabetes',\n",
        "       'Alcoholism', 'SMS_received', 'Male', 'Handcap__0',\n",
        "       'Handcap__1', 'Handcap__2', 'Handcap__3', 'Handcap__4',\n",
        "       'Appoint Weekday__0', 'Appoint Weekday__1', 'Appoint Weekday__2',\n",
        "       'Appoint Weekday__3', 'Appoint Weekday__4', 'Appoint Weekday__5',\n",
        "       'K-means cluster__0', 'K-means cluster__1', 'K-means cluster__2', 'K-means cluster__3', 'K-means cluster__4',\n",
        "       'K-means cluster__5', 'K-means cluster__6', 'K-means cluster__7', 'K-means cluster__8', 'K-means cluster__9',\n",
        "       'K-means cluster__10', 'K-means cluster__11', 'K-means cluster__12', 'K-means cluster__13',\n",
        "       'K-means cluster__14', 'age_scaled', 'New_Visit', 'Noshow_rate']\n",
        "\n",
        "\n",
        "X_train_os = train_set_os[features]\n",
        "y_train_os = train_set_os['Label']\n",
        "\n",
        "X_val_os = val_set_os[features]\n",
        "y_val_os = val_set_os['Label']\n",
        "\n",
        "\n",
        "X_train = train_set[features]\n",
        "y_train = train_set['Label']\n",
        "\n",
        "X_val = val_set[features]\n",
        "y_val = val_set['Label']\n",
        "\n",
        "X_test = test_set[features]\n",
        "y_test = test_set['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6IHz4TxzVXS",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4eCUqR5zXhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def lr(X, y):\n",
        "  time_start = datetime.now()\n",
        "  name =[x for x in globals() if globals()[x] is X][0]\n",
        "\n",
        "  logreg = LogisticRegression()\n",
        "  logreg.fit(X, y)\n",
        "  y_pred = logreg.predict(X_test)\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(f'\\n\\n Accuracy of logistic regression classifier on test set: {round(logreg.score(X_test, y_test),2)} -- { name}')\n",
        "  print(cm)\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  time_end = datetime.now()\n",
        "  time_total = (time_end - time_start).total_seconds()\n",
        "  print(\"\\n run time: \", time_total, \"\\n\")\n",
        "\n",
        "\n",
        "lr(X_train, y_train)\n",
        "lr(X_train_os, y_train_os)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61dyjd07zbet",
        "colab_type": "text"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92tYEABuzdxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "def rf(X, y):\n",
        "  time_start = datetime.now()\n",
        "  name =[x for x in globals() if globals()[x] is X][0]\n",
        "\n",
        "  rf = RandomForestClassifier()\n",
        "  rf.fit(X, y)\n",
        "  y_pred = rf.predict(X_test)\n",
        "  print(f\"\\n\\n random forest -- {name}\")\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  fi = pd.DataFrame({'feature': list(X.columns),\n",
        "                   'importance': rf.feature_importances_}).\\\n",
        "                    sort_values('importance', ascending = False)\n",
        "  print(\"feature importance: \", fi.head(10))\n",
        "  \n",
        "  time_end = datetime.now()\n",
        "  time_total = (time_end - time_start).total_seconds()\n",
        "  print(\"\\n run time: \", time_total, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "rf(X_train, y_train)\n",
        "rf(X_train_os, y_train_os)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2pe5p-Bzgtd",
        "colab_type": "text"
      },
      "source": [
        "#### SVM\n",
        "\n",
        "Be careful, the algorithm of SVM will take so long time to run with a large dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xawXObBYzi0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "def svm(X, y, kernel):\n",
        "  time_start = datetime.now()\n",
        "  name =[x for x in globals() if globals()[x] is X][0]\n",
        "  \n",
        "  df = pd.concat([X, y], axis=1).sample(frac = .5, random_state = 8)\n",
        "  X = df[features]\n",
        "  y = df['Label']  \n",
        "\n",
        "  svclassifier = SVC(kernel=kernel)\n",
        "  svclassifier.fit(X, y)\n",
        "  y_pred=svclassifier.predict(X_test)\n",
        "  print(f'\\n\\n  SVM: {kernel} kernel -- {name} \\n {confusion_matrix(y_test, y_pred)}')\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  time_end = datetime.now()\n",
        "  time_total = (time_end - time_start).total_seconds()\n",
        "  print(\"\\n run time: \", time_total, \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# svm(X_train, y_train, \"linear\")\n",
        "# svm(X_train_os, y_train_os, \"linear\")\n",
        "\n",
        "svm(X_train, y_train, \"poly\")\n",
        "svm(X_train_os, y_train_os, \"poly\")\n",
        "\n",
        "\n",
        "svm(X_train, y_train, \"rbf\")\n",
        "svm(X_train_os, y_train_os, \"rbf\")\n",
        "\n",
        "svm(X_train, y_train, \"sigmoid\")\n",
        "svm(X_train_os, y_train_os, \"sigmoid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjKOacjCzmDc",
        "colab_type": "text"
      },
      "source": [
        "# Use Neural Network to train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6Q5ovXkzo_B",
        "colab_type": "text"
      },
      "source": [
        "Training session. Tune the hyperparameters with validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_OuItnMzvus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:90% !important; }</style>\"))  \n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "\"\"\"Setup the network\"\"\"\n",
        "class mydata(data.Dataset):\n",
        "   \n",
        "    def __init__(self, df, label_name):\n",
        "        #df = pd.read_csv(data_file_path)\n",
        "        labels = df[label_name].values\n",
        "        data = df.drop(label_name, axis=1)\n",
        "        self.datalist = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(np.asarray(self.datalist.iloc[[index]].astype(float))), self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datalist.shape[0]\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, dropout_rate = 0.02):\n",
        "        super().__init__()      \n",
        "        self.hidden1 = nn.Linear(36, 25)     \n",
        "        self.hidden2 = nn.Linear(25, 10)      \n",
        "        self.output = nn.Linear(10, 2)\n",
        "\n",
        "        self.sigmoid1 = nn.Sigmoid()\n",
        "        self.sigmoid2 = nn.Sigmoid()\n",
        "        # self.sigmoid =nn.Sigmoid()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "      \n",
        "        self.batchnorm1 = nn.BatchNorm1d(25)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
        "   \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.sigmoid1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "\n",
        "        x = self.hidden2(x)\n",
        "        x = self.sigmoid2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "        # x = self.sigmoid(x)    \n",
        "        return x\n",
        "    \n",
        "\"\"\"hyperparameters\"\"\"\n",
        "loss_func = nn.NLLLoss()\n",
        "loss_func_name = 'neg log like'\n",
        "\n",
        "# loss_func = nn.CrossEntropyLoss()\n",
        "# loss_func_name = 'cross entropy'\n",
        "learn_rate = .001\n",
        "num_epochs = 1000\n",
        "batch_size = 1024\n",
        "confidence_threshold = 0.5\n",
        "loss_adj_conf_thresh = np.log(confidence_threshold)\n",
        "optimizer_name = 'Adam'\n",
        "start_time = datetime.now()\n",
        "\n",
        "\"\"\"export training log\"\"\"\n",
        "run_id = f\"hp run_{datetime.now().hour}_{datetime.now().minute}\"\n",
        "try:\n",
        "    os.mkdir(run_id)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "\n",
        "with open(run_id + '/hyperparams.csv', 'w') as wfil:\n",
        "    wfil.write(\"loss function,\" + loss_func_name + '\\n')\n",
        "    wfil.write(\"learning rate,\" + str(learn_rate) + '\\n')\n",
        "    wfil.write(\"number epochs,\" + str(num_epochs) + '\\n')\n",
        "    wfil.write(\"batch size,\" + str(batch_size) + '\\n')\n",
        "    wfil.write(\"optimizer,\" + str(learn_rate) + '\\n')\n",
        "    wfil.write(\"start time,\" + str(start_time) + '\\n')\n",
        "\n",
        "\"\"\"load data\"\"\"\n",
        "train_set = mydata(train_set_os, 'Label')\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    dataset=train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_set = mydata(val_set_os, 'Label')\n",
        "validloader = torch.utils.data.DataLoader(\n",
        "    dataset=valid_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\"\"\"detect device, this code can be used on GPU \"\"\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Network()\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "model = model.to(device)    \n",
        "\n",
        "print('device is :', device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learn_rate)  # The optimizer is Adam \n",
        "\n",
        "best_valid_loss = \"unset\"\n",
        "\n",
        "\"\"\"Training\"\"\"\n",
        "with open(run_id + '/log_file.csv', 'w') as log_fil: \n",
        "    \n",
        "    log_fil.write(\n",
        "        \"epoch,epoch duration,train loss,valid loss,test loss,train accuracy, valid accuracy, test accuracy,  train precision, valid precision, test precision, train recall, valid recall, test recall\\n\")\n",
        "\n",
        "    for epoch in range(0, num_epochs):\n",
        "        epoch_start = datetime.now()\n",
        "   \n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_valid_loss = 0.0\n",
        "    \n",
        "        epoch_train_accuracy = 0.0\n",
        "        epoch_valid_accuracy = 0.0\n",
        "        epoch_train_counter = 0.0\n",
        "        epoch_valid_counter = 0.0\n",
        "           \n",
        "        epoch_train_tp = 0.0\n",
        "        epoch_valid_tp = 0.0\n",
        "      \n",
        "        epoch_train_tn = 0.0\n",
        "        epoch_valid_tn = 0.0      \n",
        "\n",
        "        epoch_train_fp = 0.0\n",
        "        epoch_valid_fp = 0.0\n",
        "\n",
        "        epoch_train_fn = 0.0\n",
        "        epoch_valid_fn = 0.0\n",
        "      \n",
        "        epoch_train_precision = 0.0\n",
        "        epoch_valid_precision = 0.0      \n",
        "\n",
        "        epoch_train_recall = 0.0\n",
        "        epoch_valid_recall = 0.0\n",
        "       \n",
        "\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "            \n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)           \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "#             print(\"Outside: input size\", labels.size(), \"output_size\", outputs.size())         \n",
        "            loss = loss_func(outputs, labels)     \n",
        "            loss.backward()          \n",
        "            optimizer.step()\n",
        " \n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            \n",
        "            for i in range(0, len(labels)):\n",
        "                temp_label = labels[i]\n",
        "                temp_pred = outputs[i, 1]\n",
        "                \n",
        "                if temp_pred > loss_adj_conf_thresh:\n",
        "                    temp_pred = 1.0\n",
        "                else:\n",
        "                    temp_pred = 0.0\n",
        "                \n",
        "                if float(temp_pred) - float(temp_label) == 0:\n",
        "                    epoch_train_accuracy += 1.0\n",
        "\n",
        "                epoch_train_counter += 1.0\n",
        "\n",
        "                if float(temp_label) == 1.0 and float(temp_pred) == 1.0:\n",
        "                    epoch_train_tp += 1.0\n",
        "\n",
        "                if float(temp_label) == 0.0 and float(temp_pred) == 0.0:\n",
        "                    epoch_train_tn += 1.0\n",
        "\n",
        "                if float(temp_label) == 0.0 and float(temp_pred) == 1.0:\n",
        "                    epoch_train_fp += 1.0\n",
        "\n",
        "                if float(temp_label) == 1.0 and float(temp_pred) == 0.0:\n",
        "                    epoch_train_fn += 1.0\n",
        "\n",
        "        epoch_train_accuracy = epoch_train_accuracy / epoch_train_counter\n",
        "        try:\n",
        "            epoch_train_precision = epoch_train_tp / \\\n",
        "                (epoch_train_tp + epoch_train_fp)\n",
        "        except ZeroDivisionError:\n",
        "            epoch_train_precision = None\n",
        "\n",
        "        try:\n",
        "            epoch_train_recall = epoch_train_tp / \\\n",
        "                (epoch_train_tp + epoch_train_fn)\n",
        "        except ZeroDivisionError:\n",
        "            epoch_train_recall = None\n",
        "            \n",
        " \n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (images, labels) in enumerate(validloader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = loss_func(outputs, labels)\n",
        "                epoch_valid_loss += loss.item()\n",
        "\n",
        "                \n",
        "                for i in range(0, len(labels)):\n",
        "                    temp_label = labels[i]\n",
        "                    temp_pred = outputs[i, 1]\n",
        "                    \n",
        "                    if temp_pred > loss_adj_conf_thresh:\n",
        "                        temp_pred = 1.0\n",
        "                    else:\n",
        "                        temp_pred = 0.0\n",
        "\n",
        "                    if float(temp_pred) - float(temp_label) == 0:\n",
        "                        epoch_valid_accuracy += 1.0\n",
        "                    epoch_valid_counter += 1.0\n",
        "\n",
        "                    if float(temp_label) == 1.0 and float(temp_pred) == 1.0:\n",
        "                        epoch_valid_tp += 1.0\n",
        "\n",
        "                    if float(temp_label) == 0.0 and float(temp_pred) == 0.0:\n",
        "                        epoch_valid_tn += 1.0\n",
        "\n",
        "                    if float(temp_label) == 0.0 and float(temp_pred) == 1.0:\n",
        "                        epoch_valid_fp += 1.0\n",
        "\n",
        "                    if float(temp_label) == 1.0 and float(temp_pred) == 0.0:\n",
        "                        epoch_valid_fn += 1.0\n",
        "\n",
        "        epoch_valid_accuracy = epoch_valid_accuracy / epoch_valid_counter\n",
        "        try:\n",
        "            epoch_valid_precision = epoch_valid_tp / \\\n",
        "                (epoch_valid_tp + epoch_valid_fp)\n",
        "        except ZeroDivisionError:\n",
        "            epoch_valid_precision = None\n",
        "        try:\n",
        "            epoch_valid_recall = epoch_valid_tp / \\\n",
        "                (epoch_valid_tp + epoch_valid_fn)\n",
        "        except ZeroDivisionError:\n",
        "            epoch_valid_recall = None\n",
        "                    \n",
        "                     \n",
        "        epoch_end = datetime.now()\n",
        "        epoch_time = (epoch_end - epoch_start).total_seconds()\n",
        "\n",
        "        \n",
        "        if best_valid_loss == \"unset\" or epoch_valid_loss < best_valid_loss:\n",
        "            best_valid_loss = epoch_valid_loss\n",
        "            torch.save(model, run_id + \"/best_weights.pth\")\n",
        "\n",
        "        \n",
        "        torch.save(model, run_id + \"/last_weights.pth\")\n",
        "\n",
        "        \n",
        "        log_fil.write(str(epoch) + ',' + str(epoch_time) + ','\n",
        "                      + str(epoch_train_loss) + ','\n",
        "                      + str(epoch_valid_loss) + ','\n",
        "                      \n",
        "                      + str(epoch_train_accuracy) + ','\n",
        "                      + str(epoch_valid_accuracy) + ','\n",
        "                      \n",
        "                      + str(epoch_train_precision) + ','\n",
        "                      + str(epoch_valid_precision) + ','\n",
        "                      \n",
        "                      + str(epoch_train_recall) + ','\n",
        "                      + str(epoch_valid_recall) \n",
        "                     \n",
        "                      + '\\n')\n",
        "\n",
        "       \n",
        "        print(\"epoch: \" + str(epoch) + \" - (\" + str(epoch_time) + \" seconds)\" + \"\\n\\ttrain loss: \" + str(epoch_train_loss)[:5] \n",
        "              + \" - train accuracy: \" + str(epoch_train_accuracy)[:5]  \n",
        "              + \" - train precision \" + str(epoch_train_precision)[:5]  \n",
        "              + \" - train recall: \" + str(epoch_train_recall)[:5]  \n",
        "              \n",
        "\n",
        "              + \"\\n\\tvalid loss: \" + str(epoch_valid_loss)[:5]  \n",
        "              + \" - valid accuracy: \" + str(epoch_valid_accuracy)[:5] \n",
        "              + \" - valid precision \" + str(epoch_valid_precision)[:5]  \n",
        "              + \" - valid recall: \" + str(epoch_valid_recall)[:5]  \n",
        "         \n",
        "             )                                                    \n",
        "\n",
        "end_time = datetime.now()\n",
        "with open(run_id + '/hyperparams.csv', 'a') as wfil:\n",
        "    wfil.write(\"end time,\" + str(end_time) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azow3DjRzxXX",
        "colab_type": "text"
      },
      "source": [
        "Get final results by test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ree_0cmdz3B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "class mydata(data.Dataset):\n",
        "   \n",
        "    def __init__(self, df, label_name):\n",
        "        labels = df[label_name].values\n",
        "        data = df.drop(label_name, axis=1)\n",
        "        self.datalist = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.Tensor(np.asarray(self.datalist.iloc[[index]].astype(float))), self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datalist.shape[0]\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, dropout_rate = 0.02):\n",
        "        super().__init__()      \n",
        "        self.hidden1 = nn.Linear(36, 25)     \n",
        "        self.hidden2 = nn.Linear(25, 10)      \n",
        "        self.output = nn.Linear(10, 2)\n",
        "\n",
        "        self.sigmoid1 = nn.Sigmoid()\n",
        "        self.sigmoid2 = nn.Sigmoid()\n",
        "        # self.sigmoid =nn.Sigmoid()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
        "      \n",
        "        self.batchnorm1 = nn.BatchNorm1d(25)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(10)\n",
        "   \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.hidden1(x)\n",
        "        x = self.sigmoid1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "\n",
        "        x = self.hidden2(x)\n",
        "        x = self.sigmoid2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = self.softmax(x)\n",
        "        # x = self.sigmoid(x)    \n",
        "        return x\n",
        "    \n",
        "# identify where the weights you want to load are \n",
        "weight_fil = \"../input/no-show/best_weights.pth\"\n",
        "\n",
        "# set necessary hyperparameters\n",
        "batch_size = 1024\n",
        "loss_func = nn.NLLLoss()\n",
        "confidence_threshold = 0.5\n",
        "loss_adj_conf_thresh = np.log(confidence_threshold)\n",
        "\n",
        "\n",
        "# # initialize model\n",
        "# model = Network()\n",
        "\n",
        "# # load weights\n",
        "# model.load_state_dict(torch.load(weight_fil))\n",
        "\n",
        "model = torch.load(weight_fil)\n",
        "\n",
        "# put model in evaluation mode (sets dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results)\n",
        "model.eval()\n",
        "\n",
        "# create loaders to feed in data to the network in batches\n",
        "drops = ['No-show',\"Gender\",'AppointmentID', 'PatientId', 'ScheduledDay','AppointmentDay', 'Age','under18', 'Neighbourhood']\n",
        "test_set = test_set.drop(drops, axis=1).copy()\n",
        "eval_set = mydata(test_set, 'Label')\n",
        "eval_loader = torch.utils.data.DataLoader( dataset = eval_set , batch_size= batch_size , shuffle = True)\n",
        "\n",
        "# track metrics over dataset\n",
        "eval_loss = 0.0\n",
        "eval_counter = 0.0\n",
        "eval_accuracy = 0.0\n",
        "eval_precision = 0.0\n",
        "eval_recall = 0.0\n",
        "    \n",
        "eval_tp = 0.0\n",
        "eval_tn = 0.0\n",
        "eval_fp = 0.0\n",
        "eval_fn = 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# loop through eval data\n",
        "for i, (images, labels) in enumerate(eval_loader):\n",
        "    outputs = model(images)\n",
        "    loss = loss_func(outputs, labels)\n",
        "    eval_loss += loss.item()\n",
        "    for i in range(0, len(labels)):\n",
        "        temp_label = labels[i]\n",
        "        temp_pred = outputs[i, 1]\n",
        "      \n",
        "        if temp_pred > loss_adj_conf_thresh:\n",
        "            temp_pred = 1.0\n",
        "        else:\n",
        "            temp_pred = 0.0\n",
        "    \n",
        "        if float(temp_pred) - float(temp_label) == 0:\n",
        "            eval_accuracy += 1.0\n",
        "\n",
        "        eval_counter += 1.0\n",
        "\n",
        "        if float(temp_label) == 1.0 and float(temp_pred) == 1.0:\n",
        "            eval_tp += 1.0\n",
        "\n",
        "        if float(temp_label) == 0.0 and float(temp_pred) == 0.0:\n",
        "            eval_tn += 1.0\n",
        "\n",
        "        if float(temp_label) == 0.0 and float(temp_pred) == 1.0:\n",
        "            eval_fp += 1.0\n",
        "\n",
        "        if float(temp_label) == 1.0 and float(temp_pred) == 0.0:\n",
        "            eval_fn += 1.0\n",
        "\n",
        "eval_accuracy = eval_accuracy / eval_counter\n",
        "try:\n",
        "    eval_precision = eval_tp / \\\n",
        "        (eval_tp + eval_fp)\n",
        "except ZeroDivisionError:\n",
        "    eval_precision = None\n",
        "\n",
        "try:\n",
        "    eval_recall = eval_tp / \\\n",
        "        (eval_tp + eval_fn)\n",
        "except ZeroDivisionError:\n",
        "    eval_recall = None\n",
        "\n",
        "print(\"Accuracy = \" + str(eval_accuracy))\n",
        "print(\"Loss = \" + str(eval_loss))\n",
        "print(\"Precision = \" + str(eval_precision))\n",
        "print(\"Recall = \" + str(eval_recall))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mua_Xr3z4Pw",
        "colab_type": "text"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "Random forest model had the best overall performance in our project. We ended up with a predicting model, a random forest model, with 0.83 precision, 0.73 recall, and 0.91 accuracy on our testing set, which exceeds almost all current published no-show prediction models. However, considering the real world application, though the accuracy is satisfying, the precision and recall are still not good enough. We tried all kinds of stuff to improve precision and recall, yet no big improvement was achieved. We identified two major limitations our models were suffering from that might account for the not-good-enough precision and recall. First, we only have 13 independent variables in our dataset. That means too few features were put into our models. Second, more than 50% patients only showed up once in our dataset thus we actually lacked more than half of the population's previous no-show information, which according to other studies is super important as a predictor. Due to the two major limitations in the dataset, we had difficulty in reaching better precision and recall. In the future studies, after including more features as well as having a longer time frame in the datasets, we believe a model with better precision and recall could be achieved which could assist designing interventions to address the appointment no-show issue to obtain an optimal allocation of scarce medical resources."
      ]
    }
  ]
}